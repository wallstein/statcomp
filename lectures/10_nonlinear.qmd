---
title: "Modern Statistical Computing"
subtitle: "10. Non-linear models"
author: "David Rossell"
institute: "Pompeu Fabra University"
execute:
  echo: true
format:
  revealjs:
    theme: [default, custom.scss]
    scrollable: true
toc: true
toc-depth: 1
number-sections: false
mouse-wheel: true
code-overflow: scroll
code-line-numbers: false
code-copy: true
cache: true
title-slide-style: pandoc
bibliography: references.bib
---

## Reproducing these lecture notes

Load required R packages

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(keras)
library(mlbench)
library(mgcv)
library(tensorflow)
library(reshape2)
#install_tensorflow()  #run this the first time you use tensorflow
source("../code/routines.R")
```


## Non-linear models

In some applications it can be critical to capture non-linear effects. We already saw two basic strategies

- Add quadratic / polynomial terms $x_{ij}^2, x_{ij}^3, \ldots$

- Discretize $x_{ij}$ into several groups

More refined strategies (e.g. used by `ggplot`)

- Additive regression

- Deep learning

- Regression trees / random forests

- ...




# Generalized additive models

---

Additive linear regression. $y_i \sim N(\mu_i, \sigma^2)$,

$$ y_i = \sum_{j=1}^p f_j(x_{ij}) + \epsilon_i= \mu_i + \epsilon_i $$


Additive logistic regression. $y_i \sim \mbox{Bern}(\mu_i)$


$$ \log \left( \frac{\mu_i}{1 - \mu_i} \right) = \sum_{j=1}^p f_j(x_{ij}) $$

Additive Poisson regression. $y_i \sim \mbox{Poisson}(\mu_i)$

$$ \log \mu_i = \sum_{j=1}^p f_j(x_{ij}) $$

## Example

GAMs can be fitted with function `gam` (package `mgcv`)

```{r}
n= 50; x= seq(0,2*pi,length=n); y= sin(x) + rnorm(n, sd=.2)
fit= gam(y ~ s(x))
```

:::panel-tabset

### Model summary

```{r}
summary(fit)
```

### Plot

```{r}
plot(x, y); lines(x, sin(x), col='blue'); lines(x, predict(fit))
legend('topright',c("Truth","Estimated"),lty=1,col=c("blue","black"))
```

:::




## Additive models via basis functions

Idea: transform $x_{ij} \in \mathbb{R}$ into vector $w_j(x_{ij}) \in \mathbb{R}^L$. Let
 
$$
\sum_{j=1}^p f_j(x_{ij})= \sum_{j=1}^p w_j(x_{ij})^T \beta_j
$$

**Examples.**

- Quadratic terms. $w_j(x_{ij})^T= (x_{ij}, x_{ij}^2)$

- Discretize into $K$ groups. $w_j(x_{ij})^T= (0,\ldots,0,1,0,\ldots,0)$ (1 indicates $x_{ij}$'s group)

More advanced examples: splines, Fourier basis etc.


## Splines


**Def.** Let $x \in \mathbb{R}$. $f(x)$ is a spline of degree $d$ and knots $\nu_1 < \ldots < \nu_K$ iff

- $f(x)$ is a degree $d$ polynomial in each interval $(\nu_k,\nu_{k+1})$

- $f(x)$ has $d-1$ continuous derivatives at $\nu_1,\ldots,\nu_K$

It's easy to obtain splines. Consider
$$f(x)= \sum_{l=1}^L w_l(x) \beta_l$$ 
where $w_l$'s are degree $d$ polynomials and $\beta_l$'s satisfy certain restrictions, ensuring that $f(x)$ has $d-1$ continuous derivatives


## Example. Degree 1 B-splines

$$f(x)= w_1(x) + 1.1 w_2(x) +1.5 w_3(x) + 1.6 w_4(x)$$

B-splines: minimal support & guarantee $d-1$ continuous derivatives

::: {layout-ncol=2}

![](figs/splinelinear_basis.jpeg)

![](figs/splinelinear_fx.jpeg)

:::


## Model fitting

Since $\sum_{j=1}^p f_j(x_{ij})= \sum_{j=1}^p w_j(x_{ij})^T \beta_j$, we can write
$$
y= W \beta + \epsilon
$$
where $y, \epsilon \in \mathbb{R}^n$, $W$ contains all the $w_j(x_{ij})$'s and $\beta^T= (\beta_1^T,\ldots,\beta_p^T) \in \mathbb{R}^{pL}$ 

A standard linear regression model!

- In principle, we could use least-squares

- Many parameters ($pL$), use generalized cross-validation to avoid over-fitting

GCV is a computationally faster alternative to cross-validation


## Example. Diamonds data

```{r}
#| code-fold: true

diamonds2= mutate(diamonds, lprice=log2(price), lcarat=log2(carat))
ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_hex(bins = 50) +
  geom_smooth(method='lm') +
  geom_smooth(color='black') +
  labs(x='log2 carats', y='log2 diamond price')
```

---

```{r}
fit1= gam(lprice ~ lcarat + cut + color + clarity, data= diamonds2)     #linear regression
fit2= gam(lprice ~ s(lcarat) + cut + color + clarity, data= diamonds2)  #GAM
```

:::panel-tabset

### fit1

```{r}
summary(fit1)
```

### fit2

```{r}
summary(fit2)
```

:::


## Exercise

Produce a residuals $\hat{y} - y$ vs. predictions $\hat{y}$ plot for the linear and GAM models


1. Does the GAM improve the linearity assumption relative to the linear model?
In particular, discuss any systematic biases in over- or under-predicting some of the diamonds

2. Does the GAM improve the error normality assumption?

Getting started

```{r, eval=FALSE}
library(tidyverse)
library(mgcv)
diamonds2= mutate(diamonds, lprice=log2(price), lcarat=log2(carat))
```




# Deep learning

##

Neural networks can capture very general non-linear patterns

```{r, echo=FALSE}
library(igraph)

#Define node locations
layout= cbind(c(0,0,rep(1,10),rep(2,10),3), c(-2,2,-4:5,-4:5,0))

#Define edges between multiple layers
edge1= cbind(rep(1:2,each=10),rep(3:12,2))
edge2= as.matrix(expand.grid(3:12,13:22))
edge2= edge2[edge2[,1] < edge2[,2],]
edge3= cbind(rep(13:22), 23)
edge= rbind(edge1, edge2, edge3)

#Plot network
g= graph.data.frame(edge, directed=TRUE)
par(mar=rep(0,4), oma=rep(0,4))
plot(g, layout=layout, vertex.label=NA, xlim=c(-1,1), ylim=c(-1.1,1.1), asp=0)
text(-1,1.2,"Covariates", cex=1.25)
text(-0.33,1.2,"Layer 1", cex=1.25)
text(0.33,1.2,"Layer 2", cex=1.25)
text(1,1.2,"Output", cex=1.25)
```


## In a nutshell

Value of node $k$ in layer $j$: linear combination of incoming nodes from layer $j-1$, apply an *activation function*

Let $x^{(0)}$ be the input covariates. For layer $k$, nodes $j=1,\ldots,N_k$ are
$$ x_j^{(k)}= g\left( \beta_{kj0} + \beta_{kj}^T x^{(k-1)} \right)$$

Popular activation functions $g()$: ReLu, softmax, sigmoid

Last layer $K+1$ has 1 node: the outcome
$\hat{y}= \beta_{K+1, 0} + w^T_{K+1} x^{(K)}$

Finally, specify loss function: least-squares, logistic regression log-likelihood


## Practical issues

- Many parameters ($K$ layers with $N$ nodes has order $p N + K N^2$)

- Many tuning parameters: number of nodes, layers. Train/test data split

- Loss function has many local modes. We'll trust stochastic gradient descent (run the model several times for safety)


Further resources

- [Getting started with deep learning in R](https://posit.co/blog/getting-started-with-deep-learning-in-r) guide

- [Deep learning book](https://srdas.github.io/DLBook) (Chapter 10)



## Breast cancer data

As illustration we use a binary outcome example, see [here](https://tensorflow.rstudio.com/tutorials/keras/regression) for a continuous outcome example

- Outcome: malignant breast cancer

- 9 covariates measuring cell characteristics

- Sample size $n=699$


## Pre-processing

Convert to numeric, standardize covariates to mean 0 & variance 1 (important!), train/test split

```{r}
data("BreastCancer")  #package mlbench
BreastCancer = BreastCancer[which(complete.cases(BreastCancer)==TRUE),] #exclude cases with missing values
BreastCancer = mutate(BreastCancer, Class= ifelse(Class=='malignant',1,0)) |>
  mutate_if(is.factor, as.numeric) |> #convert factors to numeric
  select(-Id)  #drop Id column

covars= names(select(BreastCancer, -Class))
BreastCancer= mutate_at(BreastCancer, covars, scale) #standardize to mean 0, variance 1

n= nrow(BreastCancer)
sel= sample(1:n, size=round(0.8*n), replace=FALSE)  #80% of data in training set, 20% in test set
train= BreastCancer[sel,]
test= BreastCancer[-sel,]

Xtrain= data.matrix(select(train, -Class))
ytrain= data.matrix(select(train, Class))

Xtest= data.matrix(select(test, -Class))
ytest= data.matrix(select(test, Class))

```


## Setting up Tensorflow

Define the model: 2 hidden layers with 100 units each

```{r}
nunits= 100

model= keras_model_sequential() |>
  layer_dense(units= nunits, activation='relu') |>
  layer_dense(units= nunits, activation='relu') |>
  layer_dense(units=1, activation='sigmoid')
```

Compile the model

```{r}
model |> compile(
  loss = 'binary_crossentropy',    #logistic regression loss
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

Fit the model

```{r}
model |> fit(Xtrain, ytrain, epochs = 100, verbose = 0, validation_split = 0.1)
```

## Result accuracy

```{r}
ytestpred= predict(model, Xtest)

boxplot(ytestpred ~ ytest, xlab='Malignant tumor', ylab='Deep learning prediction')
```

```{r}
table(ytestpred > 0.5, ytest)
```


---

```{r}
pred= data.frame(predict(model, Xtrain)[,1], Xtrain) |> as_tibble()
names(pred)= c("ypred",colnames(Xtrain))
```


```{r}
ggplot(pred, aes(Cl.thickness, ypred)) + geom_point() + geom_smooth()
```

## All covariates

Trick: re-format data and facet by covariate

```{r}
pred
```


```{r}
predlong= melt(pred, id.vars="ypred")  #from package reshape2
predlong
```

---

```{r, warning=FALSE}
ggplot(predlong, aes(value, ypred)) + geom_point() + geom_smooth() + facet_wrap(~ variable)
```


## Compare with a GAM

Let's fit a logistic GAM. Variable `Mitosis` has most data in 1 category, set a linear effect to avoid `gam` returning an error


```{r}
traindata= data.frame(y=ytrain, Xtrain)
testdata= data.frame(y=ytest, Xtest)

gamfit= gam(Class ~ s(Cl.thickness) + s(Cell.size) + s(Cell.shape) + s(Marg.adhesion) + s(Epith.c.size) + s(Bare.nuclei) + s(Bl.cromatin) + s(Normal.nucleoli) + Mitoses, data=traindata, family=binomial())

gampred= expit(predict(gamfit, newdata=testdata))
```

---

:::panel-tabset


### Test set

```{r}
boxplot(gampred ~ testdata$Class, xlab='True class', ylab='GAM prediction', cex.lab=1.3)
```

### GAM vs DL (train)

```{r}
col= ifelse(ytrain[,1]==1, 'red','black')
plot(expit(predict(gamfit)), pred$ypred, xlab='GAM', ylab='Deep learning', col=col, main='Training set')
legend('topleft', c('Malign','Benign'), pch=1, col=c('red','black'))
```


### GAM vs DL (test)

```{r}
col= ifelse(ytest[,1]==1, 'red','black')
plot(gampred, ytestpred, xlab='GAM', ylab='Deep learning', col=col, main='Test set')
legend('topleft', c('Malign','Benign'), pch=1, col=c('red','black'))
```

:::


---

Both models predict very similarly in the test sample. Which one would you use?

```{r}
table(ytestpred > 0.5, ytest[,1])
```


```{r}
table(gampred > 0.5, ytest[,1])
```



