---
title: "Modern Statistical Computing"
subtitle: "5. Basic models"
author: "David Rossell"
institute: "Pompeu Fabra University"
execute:
  echo: true
format:
  revealjs:
    theme: [default, custom.scss]
    scrollable: true
toc: true
toc-depth: 1
number-sections: false
mouse-wheel: true
code-overflow: scroll
code-line-numbers: false
code-copy: true
cache: true
title-slide-style: pandoc
bibliography: references.bib
---

## Reproducing these lecture notes

Load the required R packages

```{r, warning=FALSE}
library(coefplot)
library(ggpubr)
library(gapminder)
library(hexbin)
library(modelr)
library(multcomp)
library(tidyverse)
```

We also load auxiliary routines stored at `routines.R` (directory `code`)

```{r}
source('../code/routines.R') #recall to set path to directory
```


## Models

George Box's famous mantra

> All models are wrong, but some are useful

Its less well-known context

> Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an "ideal" gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules.

> For such a model there is no need to ask the question "Is the model true?". If "truth" is to be the "whole truth" the answer must be "No". The only question of interest is "Is the model illuminating and useful?".


# Linear models

Models help interpret what's going on in a dataset

$$
y_i = \beta_0 + \sum_{j=1}^p \beta_j x_{ij} + \epsilon_i
$$
where $\epsilon_i \sim N(0,\sigma^2)$ indep $i=1,\ldots,n$

- `lm` fits a linear regression by least-squares in R

- `glm` fits generalized linear models (for non-normal outcomes, e.g. binary)

- `gam` fits generalized additive models

$$
y_i= \sum_{j=1}^p f_j(x_{ij}) + \epsilon_i \mbox{, unknown } f_j
$$


## Reminder

$$y= \begin{pmatrix} y_1 \\ \ldots \\ y_n \end{pmatrix};
X=\begin{pmatrix}
1 & x_{11} & \ldots & x_{1p} \\
\ldots \\
1 & x_{n1} & \ldots & x_{np}
\end{pmatrix}
$$

Then (assuming $X^TX$ is invertible)
$$\hat{\beta}= (X^T X)^{-1} X^T y= \arg\min_\beta \sum_{i=1}^n (y_i - x_i^T \beta)^2$$

Further, if the model assumptions hold

$$\hat{\beta} \sim N(\beta, V(\hat{\beta})); V(\hat{\beta})= \sigma^2 (X^T X)^{-1}$$
Which gives confidence intervals and P-values, e.g.
$\hat{\beta}_j \pm 1.96 \sqrt{\hat{V}(\hat{\beta_j})}$

## Reminder

Sometimes we're interested in linear combinations of parameters

**Result.** Let $Z \sim N(\mu, V)$ be a $p$-dimensional Normal distribution, and
$C$ a full-rank $q \times p$ matrix ($q \leq p$). Then
$$
W= C Z \sim N(C \mu, C V C^T)
$$
is a $q$-dimensional normal

**Example.** We seek a 95% CI for $\beta_2 - \beta_1$. Let
$C= \begin{pmatrix}
-1 & 1 & 0 & \ldots & 0 \\
\end{pmatrix}$
Then $$C \begin{pmatrix} \hat{\beta}_1 \\ \hat{\beta}_2 \\ \ldots \\ \hat{\beta}_p \end{pmatrix} = \hat{\beta}_2 - \hat{\beta}_1 \sim N(\beta_2 - \beta_1, \sigma^2 C (X^T X)^{-1} C^T)$$



## Example. Diamonds data

What drives diamond prices?
Exploration suggests that low-quality diamonds are more expensive (worst diamond color is J (yellow-ish), worst clarity is l1)

::: panel-tabset

### Price vs. cut

```{r}
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
```

### Price vs. color

```{r}
ggplot(diamonds, aes(color, price)) + geom_boxplot()
```

### Price vs. clarity

```{r}
ggplot(diamonds, aes(clarity, price)) + geom_boxplot()
```
:::


## Diagnosing the issue

Price is strongly associated with carats (diamond weight)

```{r}
ggplot(diamonds, aes(carat, price)) + 
  geom_hex(bins = 50) +
  geom_smooth(method='lm')
```

---

Carats also associated with cut, color and clarity

::: panel-tabset

### Carats vs. cut

```{r}
ggplot(diamonds, aes(cut, carat)) + geom_boxplot()
```

### Carats vs. color

```{r}
ggplot(diamonds, aes(color, carat)) + geom_boxplot()
```

### Carats vs. clarity

```{r}
ggplot(diamonds, aes(clarity, carat)) + geom_boxplot()
```
:::


---

The most important assumption in a linear model: **linearity!**

```{r}
diamonds2= mutate(diamonds, lprice=log2(price), lcarat=log2(carat))
ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_hex(bins = 50) +
  geom_smooth(method='lm')
```

log2 facilitates interpretation (+1 in log2 scale $\Rightarrow \times 2$ in original scale)

---

Fit linear model and save residuals

```{r}
lmfit= lm(lprice ~ lcarat, data = diamonds2)
diamonds2$res= residuals(lmfit)
```

Recall that worst diamond color is J (yellow-ish), worst clarity is l1

::: panel-tabset

### Residuals vs. cut

```{r}
ggplot(diamonds2, aes(cut, res)) + geom_boxplot()
```

### Residuals vs. color

```{r}
ggplot(diamonds2, aes(color, res)) + geom_boxplot()
```

### Residuals vs. clarity

```{r}
ggplot(diamonds2, aes(clarity, res)) + geom_boxplot()
```
:::



## Fitting the full model

```{r}
lmfit2= lm(lprice ~ lcarat + cut + color + clarity, data=diamonds2)
```

Second model has better $R^2$ coefficient. Careful though, comparison not fully reliable due to over-fitting (to be discussed)

```{r}
summary(lmfit)$r.squared
summary(lmfit2)$r.squared
```

Many statistically signif. coefficients in 2nd model

```{r}
summary(lmfit2)
```


---

Let's plot their predictive accuracy

```{r}
diamonds2$pred1= predict(lmfit)
diamonds2$pred2= predict(lmfit2)
diamonds2$res2= residuals(lmfit2)
```

:::panel-tabset

### Model 1

```{r}
ggplot(diamonds2, aes(x=pred1, y=lprice)) + geom_point() + geom_abline(color='blue')
```

### Model 2

```{r}
ggplot(diamonds2, aes(x=pred2, y=lprice)) + geom_point() + geom_abline(color='blue')
```

:::


# Extracting inference

---

`summary` and `confint` give $\hat{\beta}_j$, P-values for $H_0:\beta_j=0$ and confidence intervals

```{r}
summary(lmfit2)$coef
confint(lmfit2, level=0.95)
```

---

Easier to define a function that summarizes any fitted model.
I created function `coefSummary` (at `routines.R`) for that purpose

```{r}
coefSummary
```

```{r}
coefSummary(lmfit2)
```


## Organizing code

It's useful to store useful functions separately

- Put `coefSummary` in file `routines.R`. Source it when starting R

```{r, eval=FALSE}
source('../code/routines.R') #change path to directory where routines.R is stored
```


- Create your own R package (to be seen), document and share the code


Note: [tidymodels](https://www.tidymodels.org) and [parsnip](https://parsnip.tidymodels.org) provide a unified interface for many statistical/machine learning models

Note: R package `caret` does as well (see an [introduction](https://topepo.github.io/caret))


## Plotting intervals

`coefplot` helps visualize 95% confidence intervals $\hat{\beta}_j \pm 2 \sqrt{\hat{V}(\hat{\beta}_j)}$

```{r}
coefplot(lmfit2, predictors="cut") #show only parameters with name including "cut"
```


# Residual analysis

## Linear model assumptions

- Linearity

- Constant error variance

- Error normality (if $n$ large, only important for prediction)

- Uncorrelated errors

Residuals vs predicted values

```{r}
ggplot(diamonds2, aes(x=pred2, y=res2)) + geom_point()
```


## Assessing linearity

```{r}
ggplot(diamonds2, aes(pred2, res2)) +
  geom_point() +
  geom_smooth() +
  geom_abline(slope=0, intercept=0, col='gray') +
  labs(x='Model prediction', y='Residuals')
```


---

Here non-linearity seems mostly due to (log) carats

```{r}
ggplot(diamonds2, aes(lcarat, lprice)) +
  geom_point() +
  geom_smooth(method='lm', col='blue') +
  geom_smooth(col='red')
```


## Assessing constant variance

```{r}
ggplot(diamonds2, aes(x=pred2, y=res2)) + 
  geom_boxplot(mapping = aes(group = cut_width(pred2, 0.2))) +
  labs(x='Model prediction', y='Residuals')
```



## Residual normality

:::panel-tabset

### Histogram (qqplot)

```{r}
library(ggpubr)
ggplot(diamonds2, aes(x=res)) +
  geom_histogram(aes(y= ..density..)) +
  stat_overlay_normal_density(linetype = "dashed") +
  labs(x='Residuals')
```

### Histogram (base R)

```{r}
hist(scale(diamonds2$res), xlab='Residuals', prob=TRUE, main='')
xseq= seq(-4,4,length=200)
lines(xseq, dnorm(xseq))
```
### qq-plot

```{r}
ggplot(diamonds2, aes(sample=scale(res))) +
  geom_qq() +
  geom_abline(slope=1, intercept=0)
```

### qq-plot (base R)

```{r}
qqnorm(scale(diamonds2$res))
abline(0,1)
```


:::

# Interpreting the coefficients

## Factors

To interpret the coefficients of categorical variables, we must understand how they're coded

To avoid problems, let's store them as standard unordered factors

```{r}
unique(diamonds2$cut)
unique(diamonds2$color)
unique(diamonds2$clarity)
```

```{r}
diamonds2$cut= factor(diamonds2$cut, ordered=FALSE)
diamonds2$color= factor(diamonds2$color, ordered=FALSE)
diamonds2$clarity= factor(diamonds2$clarity, ordered=FALSE)
unique(diamonds2$cut)
```

## model.matrix

Check how R codes internally the variables

```{r}
x= model.matrix(~ lcarat + cut + color + clarity, data=diamonds2)
x[1:5,]
```

```{r}
unique(levels(diamonds2$cut))
```

For `cut`, "Fair" is the reference category.

---

More precisely, the model is

$$
\mbox{lprice}_i= \beta_0 + \beta_1 \mbox{good}_i + \beta_2 \mbox{very good}_i + \beta_3 \mbox{premium}_i + \beta_4 \mbox{ideal}_i + \ldots + \epsilon_i 
$$
When `cut` is fair and good we get (respectively) 

$$E(\log_2 \mbox{price}_i \mid \mbox{fair}_i=1)= \beta_0$$

$$
E(\log_2 \mbox{price}_i \mid \mbox{good}_i=1)= \beta_0 + \beta_1  
$$

Hence

$$
\frac{2^{E(\log_2 \mbox{price}_i \mid \mbox{good}_i=1)}}{2^{E(\log_2 \mbox{price}_i \mid \mbox{fair}_i=1)}}= 2^{\beta_1}
$$


##

```{r}
lmfit2= lm(lprice ~ lcarat + cut + color + clarity, data=diamonds2)
b= cbind(summary(lmfit2)$coef[,c(1,4)], confint(lmfit2))
b[,-2]= round(b[,-2],3)
b
```

---

Fair -> Good increases price by 1.08, Fair -> Very good by 1.12, etc.

```{r}
b[,-2]= 2^b[,-2]
b[,-2]= round(b[,-2],3)
b[-1,]
```



## Displaying the predictions

Functions `data_grid` and `add_predictions` in package `modelr` 

- Define grid of predictor values

- Store prediction for each

- Unspecified predictors set to mean  (continuous) or reference category (discrete)

```{r}
mygrid= data_grid(diamonds2, cut, color, clarity, .model=lmfit2)
mygrid
```

---

```{r}
lmpred2= add_predictions(mygrid, model=lmfit2)
lmpred2
```


---


```{r}
ggplot(lmpred2, aes(x=clarity, y=pred, color=color)) +
  geom_point() +
  facet_wrap(~ cut) +
  labs(y='log2 (price)')
```

## Linear contrasts

We wish to obtain a conf. int. for cut= "Ideal" - "Premium"

- $\beta_5$: mean for "Premium" - "Fair" (reference)

- $\beta_6$: mean for "Ideal" - "Fair" (reference)

- $\beta_6 - \beta_5$: mean for "Ideal" - "Premium"


```{r}
coefSummary(lmfit2)
```

---

Linear contrasts can be done in `glht` (general linear hypotheses, R package `multcomp`)

```{r}
C= matrix(0,nrow=1, ncol=length(coef(lmfit2)))
C[1,5]= -1; C[1,6]=1
C[1,1:10] #show first 10 columns
```


```{r}
PvsI= glht(lmfit2, C)
summary(PvsI)
confint(PvsI)
```


## Doing it on our own

Recall that $\hat{\beta} \sim N(\beta, V)$

```{r}
bhat= matrix(coef(lmfit2), ncol=1)
V= vcov(lmfit2)
sqrt(diag(V)) #SE's for beta's (compare to summary(lmfit2))
```

Recall that $C \hat{\beta} \sim N(C \beta, C V C^T)$. For us $C \hat{\beta}= \hat{\beta}_6 - \hat{\beta}_5$, $C\beta =\beta_6 - \beta_5$.

```{r}
dhat= C %*% bhat
se= sqrt(C %*% V %*% t(C))
se
dhat.ci= c(dhat, dhat - 1.96 * se, dhat + 1.96 * se)
round(dhat.ci, 4)
```



# Interactions

---

The effect of a covariate may depend on others

**Example.** Does effect of carats depend on cut?

```{r}
#| code-fold: true
ggplot(diamonds2, aes(lcarat, lprice, color=cut)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)
```

---

Let's fit a model with interactions. All coefficients are statistically significant

```{r}
lmfit3= lm(lprice ~ lcarat + cut + lcarat:cut, data=diamonds2)
coefSummary(lmfit3)
```

---

We just fitted the model

$$
\begin{aligned}
&\mbox{lprice}_i= \beta_0 + \beta_1 \mbox{lcarat}_i + \beta_2 \mbox{good}_i + \beta_3 \mbox{vgood}_i + \beta_4 \mbox{premium}_i + \beta_5 \mbox{ideal}_i +
\\
&
\beta_6 \mbox{lcarat}_i \mbox{good}_i + \beta_7 \mbox{lcarat}_i \mbox{vgood}_i + \beta_8 \mbox{lcarat}_i \mbox{premium}_i + \beta_9 \mbox{lcarat}_i \mbox{ideal}_i + \epsilon_i
\end{aligned}
$$

For example, when cut="fair"
$$
\mbox{lprice}_i= \beta_0 + \beta_1 \mbox{lcarat}_i
$$
and when cut="good"
$$
\mbox{lprice}_i= \beta_0 + \beta_2 + (\beta_1 + \beta_6) \mbox{lcarat}_i 
$$

How do we interpret $\beta_6$?

---

If $H_0: \beta_6=\beta_7=\beta_8=\beta_9=0$, then no interactions needed

1. Fit models with and without interactions

2. Compare with likelihood-ratio test (=ANOVA for linear models)

```{r}
lmfit4= lm(lprice ~ lcarat + cut, data=diamonds2)
anova(lmfit4, lmfit3)
```


## The full exercise

Consider all possible interactions. All P-values are highly statistically significant

```{r}
lmfull= lm(lprice ~ lcarat + cut + color + clarity + lcarat:cut + lcarat:color + lcarat:clarity, data=diamonds2)
lmdrop1= lm(lprice ~ lcarat + cut + color + clarity + lcarat:color + lcarat:clarity, data=diamonds2)
pvalue1= anova(lmdrop1, lmfull)[['Pr(>F)']][2]

lmdrop2= lm(lprice ~ lcarat + cut + color + clarity + lcarat:cut + lcarat:clarity, data=diamonds2)
pvalue2= anova(lmdrop2, lmfull)[['Pr(>F)']][2]

lmdrop3= lm(lprice ~ lcarat + cut + color + clarity + lcarat:cut + lcarat:color, data=diamonds2)
pvalue3= anova(lmdrop3, lmfull)[['Pr(>F)']][2]
c(pvalue1, pvalue2, pvalue3)
```

However, predictions are nearly identical


```{r}
summary(lmfit2)$r.squared
summary(lmfull)$r.squared
```

```{r}
diamonds2$predfull= predict(lmfull)
cor(diamonds2$pred2, diamonds2$predfull)
```

---

This happens because, after adding all covariates, estimated interaction coef. became smaller.

::: panel-tabset

### Carat & Cut

```{r, echo=FALSE}
s= coefSummary(lmfit3)
sel= grep("lcarat:",s$Parameter)
knitr::kable(s[sel,])
```

### All covariates

```{r, echo=FALSE}
s= coefSummary(lmfull)
sel= grep("lcarat:",s$Parameter)
knitr::kable(s[sel,])
```


:::

---

`multiplot` visually compares coefficients across models. Interactions lcarat:cut are smaller under the full model

```{r}
multiplot(lmfull, lmfit3, intercept=FALSE)
```


# Nested models

---

A particular type of interaction is when data are nested within units (e.g. countries), and we wanna fit a model for each unit

**Example** gapminder data (country life expectancy over time)

```{r}
library(gapminder)
gapminder
```


---

```{r}
ggplot(gapminder, aes(year, lifeExp, group = country)) +
  geom_line() + facet_wrap(~ continent)
```

---

To fit a model separately for each country we can: 

- `lm` plus interaction with country. Check which is the reference country, recover intercept/slopes for the rest

- Use `group_by` and `map_df` 

Note: better alternatives to fit nested models (random effects/hierarchical models)

First, group by country and define nested dataset: each row is a country, and column `$data` is a list of tibbles

```{r}
gm_country= group_by(gapminder, country, continent) %>% nest() 
gm_country
```

---

Second, define a function to be applied to each dataset

```{r}
fitmodel= function(df) {
  fit= lm(lifeExp ~ year, data=df)
  ans= c(coef(fit), summary(fit)$r.squared)
  names(ans)= c('b0','b1','R2')
  return(ans)
}
```

Finally, use `dplyr`'s `map_df` to apply the function to each entry of the list

```{r}
coef_gm = map_df(gm_country$data, fitmodel)
coef_gm = cbind(gm_country, coef_gm)
coef_gm
```

---

The $R^2$ coefficient is large for most countries, but pretty small for a few


:::panel-tabset

### $R^2$ histogram

```{r}
ggplot(coef_gm, aes(x=R2)) + geom_histogram()
```

### Countries

```{r}
filter(coef_gm, R2 < 0.5) %>% dplyr::select(country, continent, R2)
```
---

:::

---

Unnest the dataset and add predictions

```{r}
coef_gmu= unnest(coef_gm, cols=names(coef_gm)) 
coef_gmu$pred= coef_gmu$b0 + coef_gmu$b1 * coef_gmu$year
coef_gmu
```

---

```{r}
poorfit= filter(coef_gmu, R2 < 0.25)
ggplot(poorfit) +
  geom_line(aes(year, lifeExp)) +
  geom_line(aes(year, pred), col='red') +
  facet_wrap(~ country)
```

Can you explain any of these patterns?



