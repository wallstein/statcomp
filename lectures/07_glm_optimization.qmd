---
title: "Modern Statistical Computing"
subtitle: "7. GLMs and optimization"
author: "David Rossell"
institute: "Pompeu Fabra University"
execute:
  echo: true
format:
  revealjs:
    theme: [default, custom.scss]
    scrollable: true
toc: true
toc-depth: 1
number-sections: false
mouse-wheel: true
code-overflow: scroll
code-line-numbers: false
code-copy: true
cache: true
title-slide-style: pandoc
bibliography: references.bib
---

## Reproducing these lecture notes

Load required R packages

```{r, warning=FALSE}
library(tidyverse)
library(boot)
library(coefplot)
library(modelr)
library(openintro)
```

We also source routines.R, which has auxiliary functions

```{r}
source('../code/routines.R')
```


##

Many tasks in data analysis and decision-making require optimizing a function

We focus on cases where the function is a criterion to fit the data

**Example.** Least-squares criterion
$$
\min_\beta \sum_{i=1}^n (y_i - x_i^T \beta)^2
$$

## Maximum likelihood estimation

Given a model for the observed data $y=(y_1,\ldots,y_n)$, set parameter value $\theta$ making $y$ as "probable" as possible

**Def.** The *likelihood function* is the joint density/mass function $p(y \mid \theta)$, seen as a function of $\theta$

**Def.** The MLE is $\hat{\theta}= \arg\max_\theta p(y \mid \theta)$

Technical note: we assume that the maximum exists

## Least-squares as MLE

Let $y_i \sim N(x_i^T \beta, \sigma^2)$ indep. $i=1,\ldots,n$, then
$$
p(y \mid \beta,\sigma^2)= \prod_{i=1}^n p(y_i \mid \beta,\sigma^2)=
\frac{1}{(2\pi\sigma^2)^{n/2}} \exp \left\{ -\frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta)^2  \right\}
$$

Then $(\hat{\beta},\hat{\sigma}^2)$ obtained by maximizing
$$
\log p(y \mid \beta,\sigma^2)=
- \frac{n}{2} \log(2\pi\sigma^2) -\frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta)^2
$$

Equivalently, $\hat{\beta}$ minimizes $\sum_{i=1}^n (y_i - x_i^T \beta)^2$


## Properties

Assume that data truly generated from $p(y \mid \theta^*)$, where $\log p(y \mid \theta)$ satisfies certain regularity conditions (e.g. twice continuously differentiable) and $\theta^*$ is in the interior of the parameter space

As $n \rightarrow \infty$,
$$
\hat{\theta} \stackrel{D}{\longrightarrow} N(\theta^*, H(\theta^*)^{-1}) 
$$
$H(\theta^*)$ is the hessian of $\log p(y \mid \theta)$ (Fisher information matrix) evaluated at $\theta^*$

Hence $\hat{\theta}$ attains smallest variance among all asymptotically unbiased estimators (Cramer-Rao lower bound)


# Logistic regression

## Logistic regression

Let $Y_i \sim \mbox{Bern}(\pi_i)$, where
$$
\log \left( \frac{\pi_i}{1 - \pi_i} \right)= x_i^T \beta
\Longleftrightarrow
\pi_i = \frac{1}{1 + e^{-x_i^T \beta}}
$$
To estimate $\beta$ we consider MLE. Recall: if $Y_i \sim \mbox{Bern}(\pi_i)$,
$$
P(Y_i=y_i)= \pi_i^{y_i} (1-\pi_i)^{1-y_i} = \begin{cases} \pi_i \mbox{ if } y_i=1 \\ 1-\pi_i \mbox{ if } y_i=0 \end{cases}
$$
where $y_i$ is the observed value of $Y_i$

## Log. reg. likelihood

$$
\begin{aligned}
&\log p(y \mid \beta)= \sum_{i=1}^n \log \left( \pi_i^{y_i} (1-\pi_i)^{1-y_i} \right) \\
&=\sum_{i=1}^n y_i \log \left( \frac{\pi_i}{1-\pi_i} \right) + \log(1-\pi_i) 
=\sum_{i=1}^n y_i x_i^T \beta - \log(1 + e^{x_i^T \beta})
\end{aligned}
$$

No closed-form solution. However, 

- $\log p(y \mid \beta)$ has a negative-semidefinite hessian $H(\beta)$ (it's concave)

- Strictly concave, if $X$ has full column rank ($X^TX$ invertible)

- Many efficient numerical optimization algorithms


## Example. Spam filter

The dataset `email` (package `openintro`) has $n=3921$ emails

- $y_i$: email is spam yes/no

- $x_i$: $p=20$ covariates (number of characters, whether email had "Re:" in the subject etc.)

```{r}
email
```


## Exploratory data analysis

:::panel-tabset

### Spam vs. Number char.

```{r}
ggplot(email, aes(x=num_char)) + geom_density(aes(color=spam))
```

### Spam vs. Re: in subject

```{r}
ggplot(email) + geom_bar(aes(x=spam, y=after_stat(prop), group=1)) + facet_wrap(~ re_subj)
```
:::




##

We can fit a logistic regression, as a particular case of a Generalized Linear Model

```{r}
fit= glm(spam ~ num_char + re_subj, data=email, family=binomial())
coefSummary(fit)
```

Both coefficients are negative

- More characters --> lower spam probability

- Containing Re: in the subject --> lower spam probability


## Odds-ratios

Consider individuals $(i,j)$ with the same covariate values, except $x_{i1} \neq x_{j1}$

The odds for an email being spam are
$$
\frac{\pi_i}{1-\pi_i}= e^{x_i^T \beta}; \frac{\pi_j}{1-\pi_j}= e^{x_j^T \beta}
$$

Hence the odds-ratio is
$$
\frac{\pi_i / (1-\pi_i)}{\pi_j/(1-\pi_j)}= e^{x_i^T\beta - x_j^T \beta}=  e^{\beta_1 (x_{i1} - x_{j1})}
$$

- Increase characters by +10: $e^{-0.064 \times 10}= 0.527$

- Re: vs without Re: $e^{-2.97}= 0.051$



## Plotting the predictions

We use goodies from package `modelr`

```{r}
mygrid= data_grid(email, num_char, re_subj, .model=fit)
fitpred= add_predictions(mygrid, model=fit) #obtain logit email probabilities
fitpred$predprob= 1/(1+ exp(-fitpred$pred)) #compute email probabilities
ggplot(fitpred, aes(x=num_char,y=predprob,color=re_subj)) + geom_line() + labs(y='Spam probability', x='Number of characters')
```


## Exercise

Fit a model with an interaction `numchar:re_subj`.
For what value of `re_subj` is the estimated effect of `numchar` stronger?

What's the estimated odds ratio for +10 characters

- For the group where `re_subj` is 0?

- For the group where `re_subj` is 1?

Is the interaction statistically significant (at the usual 0.05 level)?

Plot the estimated spam probability vs. `numchar` and `re_subj`. Does it look similar to the predictions given by the model without the interaction? (previous slide)

# Convex optimization

---

Minimizing functions had a huge impact on society, business & society

**Example:** Gradient descent for neural networks: [video](https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3&ab_channel=3Blue1Brown) (first 3 minutes)

Three popular strategies to minimize a convex $f(\theta)$ (equivalently, maximize concave $-f(\theta)$)

1. Newton-Raphson algorithm

2. Gradient descent algorithm (and extensions)

3. Coordinate descent algorithm

Note: if $f(\theta)$ differentiable, we equivalently seek gradient $\nabla f(\theta)=0$


## Newton-Raphson

- Initialize $\hat{\theta}^{(0)} \in \mathbb{R}^p$, $t=0$

- Quadratic approximation to $f(\theta)$ at current guess $\hat{\theta}^{(t)}$

- Set $\hat{\theta}^{(t+1)}$ to value maximizing the quadratic approx

- Iterate until convergence

Properties

- Converges in few iterations (quadratic convergence once $\hat{\theta}^{(t)}$ close to the optimum)

- If $p$ large, each iteration is costly (matrix inverse has cost $O(p^3)$). Alternative: *gradient descent and extensions*

- If $n$ very large, evaluating gradient & hessian has cost $O(n)$. Alternative: *stochastic GD*



## Newton-Raphson example


Bivariate logistic regression example (spam data)

::: columns
::: {.column width="50%"}

![](figs/logreg_newton.png)

:::

::: {.column width="50%"}

![](figs/logreg_newton_iter2.png)

:::

:::
```{r, echo=FALSE, eval=FALSE}
## Plot 1 iter of Newton-Raphson
## Step 1. Define grid and evaluate logreg objective function
bhat= coef(glm(spam ~ num_char, data= email, family=binomial()))
bhat

th0= seq(-2.5,-1,length=40); th1= seq(-0.15,0.05,length=40)
thgrid= as.matrix(expand.grid(th0, th1))
y= as.numeric(as.character(email$spam)); X= cbind(1, email$num_char)
fgrid= double(nrow(thgrid))
for (i in 1:nrow(thgrid)) fgrid[i]= flogreg(as.vector(thgrid[i,]), y=y, X=X)
df= as_tibble(cbind(thgrid, fgrid))
names(df)= c('theta0','theta1','f')

## Step 2. Evaluate Taylor approximation
b0= c(-1.5, -0.05)
#b0= b1
fderiv= fplogreg(b0, y=matrix(y,ncol=1), X=X)
g= matrix(fderiv$g, ncol=1)
H= fderiv$H
b1= as.vector(b0 - solve(H) %*% g)
fapprox= function(theta, b0, y, X, grad, hess) flogreg(theta, y=y, X=X) + t(grad) %*% matrix(theta - b0,ncol=1) + 0.5 * matrix(theta - b0,nrow=1) %*% hess %*% matrix(theta - b0,ncol=1)

fagrid= double(nrow(thgrid))
for (i in 1:nrow(thgrid)) fagrid[i]= fapprox(as.vector(thgrid[i,]), b0=b0, y=y, X=X, grad=g, hess=H)
df= cbind(df, fa=fagrid)

## Step 3. Contour plot with quadratic approx
ggplot(df, aes(theta0, theta1)) + 
  geom_contour(aes(z=f)) + 
  geom_contour(aes(z=fa), col='black', lty=2) + 
  geom_point(aes(x=bhat[1],y=bhat[2]), col='blue') +
  geom_text(aes(x=bhat[1],y=bhat[2]), label='Optimum', nudge_y=-0.01, size=5, col='blue') +
  geom_point(aes(x=b0[1],y=b0[2]), col='black') +
  geom_text(aes(x=b0[1],y=b0[2]), label="theta^(t)", parse=TRUE, nudge_x= 0, nudge_y=0, size=6) +
  geom_point(aes(x=b1[1],y=b1[2]), col='black') +
  geom_text(aes(x=b1[1],y=b1[2]), label="theta^(t+1)", parse=TRUE, size=6)


ggsave("logreg_newton_iter2.png", path="~/github/statcomp/lectures/figs")

## Step 4. Gradient descent
f_along_grad= function(alpha, b0, grad, y, X) flogreg(as.vector(b0 - alpha*grad), y=y, X=X)

b0= c(-1.5, -0.05)
thiter= matrix(NA, nrow=100, ncol=2)
thiter[1,]= b0
fiter= double(nrow(thiter))
fiter[1]= flogreg(b0, y=y, X=X)
i=2; ftol=1
while ((i<=nrow(thiter)) & (ftol > 0.01)) {
  g= fplogreg(b0, y=matrix(y,ncol=1), X=X)$g
  g= g / sqrt(sum(g^2)) #normalize to unit length
  opt= nlm(f_along_grad, p=0.1, b0=b0, grad=g, y=y, X=X)
  alphaopt= opt$estimate #optimal step size
  fiter[i]= opt$minimum
  b0= as.vector(b0 - alphaopt * g)
  thiter[i,]= b0
  ftol= fiter[i-1] - fiter[i]
  i= i+1  
}

gd_iter= as_tibble(cbind(thiter, fiter))
names(gd_iter)= c('theta0','theta1','f')
gd_iter= filter(gd_iter, rowSums(is.na(gd_iter))==0)

## Step 5. Contour + gradient descent iterations

ggplot(df, aes(theta0, theta1)) + 
  geom_contour(aes(z=f)) + 
  geom_point(aes(x=bhat[1],y=bhat[2]), col='blue') +
  geom_text(aes(x=bhat[1],y=bhat[2]), label='Optimum', nudge_y=-0.01, size=5, col='blue') +
  geom_point(aes(theta0,theta1), data=gd_iter) +
  geom_path(aes(theta0,theta1,colour=f), data=gd_iter, arrow=arrow(length=unit(0.1,"inches"))) +
  theme(legend.position='none')

ggsave("logreg_gd.png", path="~/github/statcomp/lectures/figs")
```



## NR in more detail

Let $z= \hat{\theta}^{(t)}$, $g(\theta)=\nabla f(\theta)$ the gradient and $H(\theta)= \nabla^2 f(\theta)$ the hessian

$$
f(\theta) \approx \hat{f}(\theta)= f(z) + g(z)^T (\theta - z) + \frac{1}{2} (\theta - z)^T H(z) (\theta - z)
$$

**Recall.** Let $a \in \mathbb{R}^p$ and a $p \times p$ matrix $A$. Gradient of linear & quadratic forms

- $\nabla_\theta (a^T \theta)= a$

- $\nabla_\theta (\theta^T A \theta)= (A + A^T) \theta$ (if $A$ symmetric, then $2A \theta$)

Taking the gradient of $\hat{f}(\theta)$ wrt $\theta$ gives
$$
g(z) + H(z) (z - \theta)=0 \Rightarrow \theta= z-H^{-1}(z) g(z)
$$

If $f$ strictly convex then $H$ is strictly positive-definite, hence invertible. Else an adjustment is applied, e.g. $(H + \lambda I)^{-1}$ for small $\lambda>0$


## Gradient descent

Update $\hat{\theta}^{(t)}$ in the direction of the negative gradient. That is

$$
\hat{\theta}^{(t+1)}= \hat{\theta}^{(t)} - \alpha_t g(\hat{\theta}^{(t)})
$$
for a suitably-chosen scalar $\alpha_t > 0$. 

For example, line search: set $\alpha_t$ minimizing $f(\hat{\theta}^{(t+1)})$

- The good: each iteration has cost of order $p$

- The bad: converges slower (linear convergence)


## Gradient descent example

Here it converges in 9 iterations

![](figs/logreg_gd.png)

## Coordinate descent

- Initialize $\hat{\theta}= (\hat{\theta}_1,\ldots, \hat{\theta}_p)$

- Set $\hat{\theta}_1= \arg\min_{\theta_1} f(\theta_1, \hat{\theta}_2,\ldots, \hat{\theta}_p)$

- Set $\hat{\theta}_2= \arg\min_{\theta_2} f(\hat{\theta}_1, \theta_2,\ldots, \hat{\theta}_p)$

- $\ldots$

- Set $\hat{\theta}_p= \arg\min_{\theta_p} f(\hat{\theta}_1, \hat{\theta}_2,\ldots, \theta_p)$

Remarks

- May converge slower than NR and GD. But each iteration is cheap

- No need to exactly solve each univariate optim. Enough to set $\hat{\theta}_j$ to improve $f$


## Newton-Raphson using `nlm`

Let's do the optimization. First, store outcome and covariates. We also create `ytX` so that evaluating the log-likelihood is faster

```{r}
y= ifelse(email$spam=='1',1,0)
x2= ifelse(email$re_subj=='1',1,0)
X= cbind(intercept=1, num_char=email$num_char, re_subj=x2)
ytX= matrix(y,nrow=1) %*% X
```

Next, we define the negative log-likelihood

```{r}
flogreg= function(beta, y, X, ytX, logscale=TRUE) {
  if (missing(ytX)) ytX = matrix(y,nrow=1) %*% X
  if (any(beta != 0)) {
    Xbeta= as.vector(X %*% matrix(beta,ncol=1))
    ans= -sum(ytX * beta) + sum(log(1+exp(Xbeta)))
  } else {
    n= length(y)
    ans= n * log(2)
  }
  if (!logscale) ans= exp(ans)
  return(ans)
}
```


---

Finally we call `nlm`. If gradient/hessian not provided, they're approximated numerically

```{r}
opt1= nlm(flogreg, rep(0,3), y=y, X=X, ytX=ytX, hessian=TRUE, print.level=2)
```

## Comparison to glm

Check that the gradient is 0
```{r}
opt1$gradient #should be 0
```

The inverse hessian gives the MLE covariance matrix

```{r}
thhat= opt1$estimate
H= opt1$hessian
se= sqrt(diag(solve(H)))
```

Compare to the results from `glm`

```{r}
round(cbind(thhat, se, summary(fit)$coef[,1:2]),3)
```


## Optimization in R

- `nlm`: Newton-Raphson like

- `optim`: general optim methods (quasi-Newton, conjugate gradient...) and box constraints

- `optimx` package (function `opm`)

Package `sgd` implements stochastic gradient descent for linear/generalized linear models (and beyond)





# Poisson regression


## Poisson regression

Generalized linear models are an ample model family based on two components

1. $p(y_i \mid x_i)$ belongs to the so-called exponential family (includes Normal, Binomial, Poisson...)

2. Only the expectation of $p(y_i \mid x_i)$ depends on $x_i$, and does so via $x_i^T \beta$ (in a 1-to-1 fashion)


**Example.** Logistic regression. $y_i \sim \mbox{Bern}(\pi_i)$
$$
\pi_i = E(y_i \mid x_i)= \frac{1}{1+e^{-x_i^T \beta}}
$$

**Example.** Poisson regression. $y_i \sim \mbox{Poi}(\mu_i)$
$$
\mu_i =E(y_i \mid x_i)= e^{x_i^T \beta}
$$

## Poisson likelihood

Recall: if $Y \sim \mbox{Poi}(\lambda)$ then $P(Y=y)= \lambda^y  e^{-\lambda} / y!$

If $y_i \sim \mbox{Poi}(e^{x_i^T \beta})$ indep $i=1,\ldots,n$,

$$
\begin{aligned}
&\log p(y \mid \beta)= \sum_{i=1}^n \log p(y_i \mid \beta)=
\sum_{i=1}^n \log \left( \frac{e^{x_i^T\beta y_i} e^{-e^{x_i^T \beta}}}{y_i!}  \right) \\
&=\sum_{i=1}^n x_i^T \beta y_i - e^{x_i^T \beta} - \log(y_i!)
\end{aligned}
$$
Log-likelihood is concave (strictly concave if $X$ has full column rank)


## Philippinnes household survey

<small> Example from the [Beyond MLR book](https://bookdown.org/roback/bookdown-BeyondMLR) </small>

Philippines household survey from 2015 (Philippines Stat. Authority)

- `total`: number of people living in household other than the head (poverty indicator)

- `location`: region where household is located

- `age`: age of household head

- `numLT5`: number in the household <5 years old

- `roof`: type of roof (Light/Salvaged Material, Strong Material)

---

```{r}
fHH= as_tibble(read.table("../datasets/fHH1.txt", header=TRUE))
fHH
```


## Exploratory data analysis

:::panel-tabset

### Barplot

```{r}
ggplot(fHH, aes(x=total)) + geom_bar()
```


### Scatterplot

```{r}
ggplot(fHH, aes(x=age, y=total)) + geom_point()
```

### Jittered scatterplot

```{r}
ggplot(fHH, aes(x=age, y=total)) + geom_point(position="jitter") + geom_smooth()
```

### By region

```{r}
ggplot(fHH, aes(x=age,y=total, color=location)) + geom_point(position="jitter") + geom_smooth()
```

### By roof

```{r}
ggplot(fHH, aes(x=age,y=total, color=roof)) + geom_point(position="jitter") + geom_smooth()
```

:::



## Data pre-processing

Age has a non-linear effect. Two simple options

- Discretize it in 5 year groups

- Add a quadratic effect

```{r}
fHH= mutate(fHH, aged= cut_width(age, 5), age2= age^2)
fHH$roof= recode(fHH$roof, `Predominantly Strong Material`="Strong", `Predominantly Light/Salvaged Material`="Light")
fHH
```


## Model fit

:::panel-tabset

### Discretized age

```{r}
fit= glm(total ~ location + roof + aged, data=fHH, family=poisson())
summary(fit)$coef
```
### Quadratic age effect

```{r}
fit2= glm(total ~ location + roof + age + age2, data=fHH, family=poisson())
summary(fit2)$coef
```

:::


## Plotting the fit

:::panel-tabset

### aged, location

```{r}
#| code-fold: true
mygrid= data_grid(fHH, aged, location, .model=fit)
fitpred= add_predictions(mygrid, model=fit)
fitpred$predmean= exp(fitpred$pred)
ggplot(fitpred, aes(x=aged,y=predmean,group=location, color=location)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Age of household head', y='Expected number of people in household')
```


### aged, roof

```{r}
#| code-fold: true
mygrid= data_grid(fHH, aged, roof, .model=fit)
fitpred= add_predictions(mygrid, model=fit)
fitpred$predmean= exp(fitpred$pred)
ggplot(fitpred, aes(x=aged,y=predmean,group=roof, color=roof)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Age of household head', y='Expected number of people in household')
```

### age2, location

```{r}
#| code-fold: true
mygrid= data_grid(fHH, age, location, .model=fit2)
mygrid= mutate(mygrid, age2= age^2)
fitpred= add_predictions(mygrid, model=fit2)
fitpred$predmean= exp(fitpred$pred)

ggplot(fitpred, aes(x=age,y=predmean,group=location,color=location)) + 
  geom_line() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Age of household head', y='Expected number of people in household') +
  ylim(0,5)
```

### age2, roof

```{r}
#| code-fold: true
mygrid= data_grid(fHH, age, roof, .model=fit2)
mygrid= mutate(mygrid, age2= age^2)
fitpred= add_predictions(mygrid, model=fit2)
fitpred$predmean= exp(fitpred$pred)

ggplot(fitpred, aes(x=age,y=predmean,group=roof,color=roof)) + 
  geom_line() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Age of household head', y='Expected number of people in household') +
  ylim(0,5)
```

:::



## Exercise

Based on the plot below, an age vs. roof interaction may be needed. Add it to the model and inspect whether it seems necessary

```{r}
ggplot(fHH, aes(x=age,y=total, color=roof)) + geom_point(position="jitter") + geom_smooth()
```



```{r, eval=FALSE, echo=FALSE}
fit3= glm(total ~ location + roof + age + age2 + age:roof + age2:roof, data=fHH, family=poisson())
coefSummary(fit3)

anova(fit2, fit3, test="Chisq")
```


## A possible solution

```{r, echo=FALSE}
fHHint= mutate(fHH, strong= (roof=='Strong'), agestrong= age*(roof=='Strong'), age2strong= age2*(roof=='Strong'))
fit4= glm(total ~ strong + age + age2 + agestrong + age2strong, data=fHHint, family=poisson())

mygrid= data_grid(fHHint, strong, age)
mygrid= mutate(mygrid, age2= age^2, agestrong= age*strong, age2strong= age^2 * strong)
mygrid= add_predictions(mygrid, model=fit4) %>% mutate(predmean= exp(pred))

ggplot(mygrid, aes(x=age,y=predmean,group=strong,color=strong)) + 
  geom_line() +
  labs(x='Age of household head', y='Expected number of people in household') +
  ylim(0,5)
```


# Residuals and over-dispersion

## Residuals in GLMs

Two popular types of residuals. Let $\hat{y}_i=\hat{E}(y_i \mid x_i)$

- Pearson residual: $(y_i - \hat{y}_i) / \sqrt{\hat{V}(y_i)}$. In logistic reg. $\hat{V}(y_i)= \hat{y}_i (1-\hat{y_i})$, in Poisson reg. $V(y_i)= \hat{y}_i$

- Deviance residual: compares the log-likelihood of $y_i$ relative to a model with perfect prediction $\hat{y}_i=y_i$

Both can be used like residuals in standard Gaussian linear regression


## Example

Take the household occupancy data with a quadratic age effect

```{r}
fHHres= mutate(fHH, pred= predict(fit2), resdev= residuals(fit2, type='deviance'), respearson= residuals(fit2, type='pearson'))
```

:::panel-tabset

### Deviance res.

```{r}
ggplot(fHHres, aes(pred, resdev)) + geom_point() + geom_smooth() + labs(x='Predicted (log)', y='Deviance residual')
```

### Pearson res.

```{r}
ggplot(fHHres, aes(pred, respearson)) + geom_point() + geom_smooth() + labs(x='Predicted (log)', y='Pearson residual')
```

### Constant variance

```{r}
fHHres= mutate(fHHres, predcut= cut_number(pred, 10))
ggplot(fHHres, aes(x=predcut, y=respearson)) + geom_boxplot()
```

:::


## Over-dispersion

Important assumption in logistic/Poisson regression: $E(y_i)$ determines $V(y_i)$

- Logistic: $V(y_i)= E(y_i) [1 - E(y_i)]$

- Poisson: $V(y_i)= E(y_i)$

If the assumed $V(y_i)$ is correct, the variance of Pearson residuals $e_i= (y_i - \hat{E}(y_i))/\sqrt{\hat{V}(y_i)}$ 

- Should be $\approx 1$

- Should be constant as $\hat{y}_i$ varies


## Over-dispersion

A popular way to estimate over-dispersion $\phi$.
$$
\hat{\phi}= \frac{\sum_{i=1}^n e_i^2}{n-p}
$$
where $e_i$ are Pearson residuals. If $\hat{\phi}$ much larger than 1, there is over-dispersion

It's possible to prove that then $\hat{\beta}$ has variance $\phi V(\hat{\beta})$, where $V(\hat{\beta})$ is the covariance given by standard theory

One may then adjust the MLE variance $V(\hat{\beta})$ into 
$$V_Q(\hat{\beta})= \hat{\phi} V(\hat{\beta})$$
where $V_Q$ stands for "quasi-likelihood"


## Example

```{r}
fit2q= glm(total ~ location + roof + age + age2, data=fHH, family=quasipoisson)
```

:::panel-tabset

### No over-dispersion

```{r}
summary(fit2)
```

### Over-dispersion

```{r}
summary(fit2q)
```

:::


## Over-dispersion

Easier to compare visually with `multiplot` from package `coefplot`

```{r}
multiplot(fit2, fit2q)
```

## Bootstrap

Function `bootGLM` at `routines.R` implements the bootstrap for GLMs

```{r}
bootGLM
```

```{r}
mleGLM
```

---

```{r}
f= formula(total ~ location + roof + age + age2)
fit2.bootci= bootGLM(fHH, formula=f, family=poisson(), level=0.95)
```

The bootstrap intervals are also wider than those from MLE theory

```{r}
allci= round(cbind(fit2.bootci[,-1], confint(fit2), confint(fit2q)), 3)
names(allci)= paste(c('Boot','Boot','MLE','MLE','Q','Q'), names(allci), sep='')
allci
```

# Exercise

## Campus crime data

Dataset `campuscrime.txt` from the [Beyond MLR book](https://bookdown.org/roback/bookdown-BeyondMLR) has number of crimes for $n=81$ institutions

- `type`: college (C) or university (U)

- `nv`: number of violent crimes for the institution in 1 year

- `enroll1000`: number of students enrolled at the school (thousands)

- `region`: region of the USA (C = Central, MW = Midwest, NE = Northeast, SE = Southeast, SW = Southwest, and W = West)

**Goal.** Are there differences between college and university, after accounting for enrollment and region? If so, provide a point estimate and a 95% interval for the ratio of expected crimes in university / college. Is there evidence for over-dispersion (if so, use an over-dispersion adjustment)

---

You can load the data as follows

```{r}
crime= as_tibble(read.table("../datasets/campuscrime.txt", header=TRUE, sep="\t"))
crime
```




